{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d41ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c645b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_get(path):\n",
    "    api_url = urljoin(\"https://pretalx.com/api/v2\", path)\n",
    "    resp = requests.get(api_url, headers={\"Authorization\": f\"Token {os.getenv('TOKEN')}\"})\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd951ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submissions():\n",
    "    \"\"\"Get all submissions, irrespective of status\"\"\"\n",
    "    submissions_path = Path(\"submissions.json\")\n",
    "    if submissions_path.exists():\n",
    "        return json.load(submissions_path.open())\n",
    "    else:\n",
    "        resp = api_get(\"events/djangocon-europe-2023/submissions/?limit=200\")\n",
    "        submissions = {\n",
    "            result[\"code\"]: result for result in resp[\"results\"]\n",
    "        }\n",
    "        with submissions_path.open(\"w\") as out:\n",
    "            json.dump(submissions, out)\n",
    "        return submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f531eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = get_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c24e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_submissions():\n",
    "    submissions = get_submissions()\n",
    "    for_export = {\n",
    "        subm['code']: {\n",
    "            \"speakers\": \", \".join(speaker[\"name\"] for speaker in subm[\"speakers\"]),\n",
    "            \"title\": subm[\"title\"],\n",
    "            \"abstract\": subm[\"abstract\"],\n",
    "        } for subm in submissions.values() if subm[\"state\"] == \"submitted\"\n",
    "    }\n",
    "    df = pd.DataFrame.from_dict(for_export, orient='index')\n",
    "    df.to_csv(\"submitted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68119a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews():\n",
    "    \"\"\"Get all individual reviews\"\"\"\n",
    "    page = 1\n",
    "    reviews = []\n",
    "    url = f\"events/djangocon-europe-2023/reviews?page={page}&limit=100\"\n",
    "    while True:\n",
    "        resp = api_get(url)\n",
    "        reviews.extend(resp[\"results\"])\n",
    "        if resp[\"next\"]:\n",
    "            url = resp[\"next\"]\n",
    "        else:\n",
    "            break\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad7d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = get_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a475e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorised_submissions():\n",
    "    \"\"\"\n",
    "    Assumes submissions have been exported with export_submissions() and categorised offline to add\n",
    "    a 'category' column\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\"submitted_with_categories.csv\",index_col=\"code\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d8f8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_by_submission(all_submissions, reviews):\n",
    "    \"\"\"\n",
    "    Collate reviews by submission, and add in the submission URL, speaker info and title, and category.\n",
    "    Return a dict of review dicts, separated by submission type.\n",
    "    \"\"\"\n",
    "    \n",
    "    categories_df = categorised_submissions()\n",
    "    \n",
    "    all_reviewers = {review[\"user\"] for review in reviews}\n",
    "    reviewer_key = {reviewer: f\"reviewer_{i}\" for i, reviewer in enumerate(all_reviewers, start=1)}\n",
    "    reviews_by_submission = {}\n",
    "    for review in reviews:\n",
    "        code = review[\"submission\"]\n",
    "        reviewer_code = reviewer_key[review[\"user\"]]\n",
    "        reviews_by_submission.setdefault(\n",
    "            code, {\"url\": f\"https://pretalx.com/orga/event/djangocon-europe-2023/submissions/{code}/reviews\"}\n",
    "        )[reviewer_code] = review[\"score\"]\n",
    "    \n",
    "    talks = {}\n",
    "    workshops = {}\n",
    "    for code in reviews_by_submission:\n",
    "        submission = all_submissions[code]\n",
    "        submission_type = submission[\"submission_type\"] if isinstance(submission[\"submission_type\"], str) else submission[\"submission_type\"][\"en\"]\n",
    "        submission_info = {\n",
    "            \"speakers\":  \", \".join([speaker[\"name\"] for speaker in submission[\"speakers\"]]),\n",
    "            \"submission_type\": submission_type,\n",
    "            \"title\": submission[\"title\"],\n",
    "            \"duration\": submission[\"duration\"],\n",
    "            \"category\": categories_df.loc[code].category\n",
    "        }\n",
    "        review_data = {**reviews_by_submission[code], **submission_info}\n",
    "        if submission_type == \"Talk\":\n",
    "            talks[code] = review_data\n",
    "        else:\n",
    "            assert submission_type == \"Workshop\"\n",
    "            workshops[code] = review_data\n",
    "\n",
    "    assert len(talks) + len(workshops) == len(reviews_by_submission)\n",
    "    return {\"talks\": talks, \"workshops\": workshops, \"reviewers\": reviewer_key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3967950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_dataframe(input_dict):\n",
    "    \"\"\"\n",
    "    Convert a review dict to a dataframe and calculate:\n",
    "    - mean\n",
    "    - median\n",
    "    - min\n",
    "    - max\n",
    "    - range\n",
    "    - total number of reviewers\n",
    "    - number of reviewers rating the min score\n",
    "    - number of reviewers rating the max score\n",
    "    - a preliminary decision based on median scores for submissions with consensus in their reviews\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(input_dict, orient='index')\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    reviewer_cols = [col for col in df.columns if col.startswith(\"reviewer_\")]\n",
    "    for col in reviewer_cols:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    df = df.assign(mean=df.loc[:, reviewer_cols].mean(axis=1, numeric_only=True))\n",
    "    df = df.assign(median=df.loc[:, reviewer_cols].median(axis=1, numeric_only=True))\n",
    "    df = df.assign(min=df.loc[:, reviewer_cols].min(axis=1, numeric_only=True))\n",
    "    df = df.assign(max=df.loc[:, reviewer_cols].max(axis=1, numeric_only=True))\n",
    "    df = df.assign(range=df.loc[:, reviewer_cols].max(axis=1, numeric_only=True) - df.loc[:, reviewer_cols].min(axis=1, numeric_only=True))\n",
    "    df = df.assign(reviewers=df.loc[:, reviewer_cols].count(axis=1, numeric_only=True))\n",
    "    \n",
    "    def min_count(row):\n",
    "        return sum(row[reviewer_cols] == row[\"min\"])\n",
    "    \n",
    "    def max_count(row):\n",
    "        return sum(row[reviewer_cols] == row[\"max\"])\n",
    "    \n",
    "    df[\"min_counts\"] = df.apply(min_count, axis=1)\n",
    "    df[\"max_counts\"] = df.apply(max_count, axis=1)\n",
    "    \n",
    "    def autodecide(row):\n",
    "        if row[\"range\"] <= 1:\n",
    "            if row[\"median\"] < 3:\n",
    "                return \"reject\"\n",
    "            if row[\"median\"] >= 4:\n",
    "                return \"accept\"\n",
    "        return \"\"\n",
    "    \n",
    "    df[\"decision_prelim\"] = df.apply(autodecide, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5443a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_df(df, decision):\n",
    "    return df[df[\"decision_prelim\"] == decision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd10062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagreed_df(df):\n",
    "    return df[df['range'] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c96783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(df):\n",
    "    from IPython.display import display, Markdown, Latex\n",
    "    display(Markdown('*some markdown* $\\phi$'))\n",
    "    print(f\"Total number of submissions: {len(df)}\\n\\n\")\n",
    "    \n",
    "    print(\"Submissions with consensus (all scores within one point)\")\n",
    "    print(\"==========================================================\")\n",
    "    agreed = df[df['range'] <= 1]\n",
    "    \n",
    "    print(f\"Total: {agreed['range'].count()}\\n\")\n",
    "    \n",
    "    counter = Counter(agreed['median'])\n",
    "    print(tabulate(sorted(counter.items()), headers=[\"Median score\", \"Count\"]))\n",
    "    \n",
    "    accepted = decision_df(df, \"accept\")\n",
    "    rejected = decision_df(df, \"reject\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Preliminarily accepted (median score >= 4): {len(accepted)}\\n\")\n",
    "    print(f\"Rejected (median score < 3): {len(rejected)}\\n\")\n",
    "\n",
    "    counter = Counter(accepted[\"speakers\"])\n",
    "    duplicate_speakers = {k: v for k, v in counter.items() if v > 1}\n",
    "    if duplicate_speakers:\n",
    "        print(\"Authors with more than one accepted submission:\\n\")\n",
    "        print(tabulate(duplicate_speakers.items(), headers=[\"Name\", \"#\"]))\n",
    "    else:\n",
    "        print(\"Authors with more than one accepted submission: None\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Submissions without consensus (scores range >= 3)\")\n",
    "    print(\"==========================================================\")\n",
    "    disagreed = disagreed_df(df)\n",
    "    print(f\"Total: {disagreed['range'].count()}\\n\")\n",
    "    \n",
    "    disagreed_scores = sorted(\n",
    "        zip(\n",
    "            disagreed[\"range\"],\n",
    "            disagreed[\"median\"], \n",
    "            disagreed[\"min\"], \n",
    "            disagreed[\"min_counts\"], \n",
    "            disagreed[\"max\"], \n",
    "            disagreed[\"max_counts\"],\n",
    "            disagreed[\"reviewers\"],\n",
    "        ),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\n",
    "        tabulate(\n",
    "            [(scores[0], scores[1], f\"{scores[2]} ({scores[3]})\", f\"{scores[4]} ({scores[5]})\", scores[6]) for scores in disagreed_scores], \n",
    "            headers=[\"Range\", \"Median\", \"Min (# reviewers)\", \"Max (# reviewers)\", \"# reviewers\"]\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a33222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_md(df):    \n",
    "    display(Markdown(f\"# {df.iloc[0].submission_type}s\"))\n",
    "    display(Markdown(f\"Total number of submissions: {len(df)}\\n\\n\"))\n",
    "    \n",
    "    display(Markdown(\"## Submissions with consensus (all scores within one point)\"))\n",
    "    \n",
    "    agreed = df[df['range'] <= 1]\n",
    "    \n",
    "    display(Markdown(f\"Total: {agreed['range'].count()}\\n\"))\n",
    "    \n",
    "    counter = Counter(agreed['median'])    \n",
    "    rows = '\\n'.join(f\"|{row[0]}|{row[1]}|\" for row in sorted(counter.items()))\n",
    "    display(Markdown(f\"|Median score|Count|\\n|--|--|\\n{rows}\"))\n",
    "    \n",
    "    accepted = decision_df(df, \"accept\")\n",
    "    rejected = decision_df(df, \"reject\")\n",
    "    display(Markdown(f\"Preliminarily accepted (median score >= 4): {len(accepted)}\\n\"))\n",
    "    display(Markdown(f\"Rejected (median score < 3): {len(rejected)}\\n\"))\n",
    "\n",
    "    counter = Counter(accepted[\"speakers\"])\n",
    "    duplicate_speakers = {k: v for k, v in counter.items() if v > 1}\n",
    "    if duplicate_speakers:\n",
    "        display(Markdown(\"Authors with more than one accepted submission:\"))\n",
    "        rows = '\\n'.join(f\"|{row[0]}|{row[1]}|\" for row in sorted(duplicate_speakers.items()))    \n",
    "        display(Markdown(f\"|Name|#|\\n|--|--|\\n{rows}\"))\n",
    "    else:\n",
    "        display(Markdown(\"Authors with more than one accepted submission: None\"))\n",
    "    \n",
    "    display(Markdown(\"## Submissions without consensus (scores range >= 3)\"))\n",
    "    disagreed = disagreed_df(df)\n",
    "    display(Markdown(f\"Total: {disagreed['range'].count()}\\n\"))\n",
    "    \n",
    "    disagreed_scores = sorted(\n",
    "        zip(\n",
    "            disagreed[\"range\"],\n",
    "            disagreed[\"median\"], \n",
    "            disagreed[\"min\"], \n",
    "            disagreed[\"min_counts\"], \n",
    "            disagreed[\"max\"], \n",
    "            disagreed[\"max_counts\"],\n",
    "            disagreed[\"reviewers\"],\n",
    "        ),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    headers = \"|Range|Median|Min (# reviewers)|Max (# reviewers)|# reviewers|\"\n",
    "    sep = \"|---|---|---|---|---|\"\n",
    "    rows = [(scores[0], scores[1], f\"{scores[2]} ({scores[3]})\", f\"{scores[4]} ({scores[5]})\", scores[6]) for scores in disagreed_scores]\n",
    "    rows = [\"|\".join(str(it) for it in row) for row in rows]\n",
    "    rows = \"\\n\".join([f\"|{row}|\" for row in rows])\n",
    "    display(Markdown(f\"{headers}\\n{sep}\\n{rows}\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e259f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_by_submissions = get_reviews_by_submission(all_submissions, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cceda1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "talks_df = summary_dataframe(reviews_by_submissions[\"talks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0553e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workshops_df = summary_dataframe(reviews_by_submissions[\"workshops\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62586b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Talks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Total number of submissions: 162\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Submissions with consensus (all scores within one point)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Total: 57\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Median score|Count|\n",
       "|--|--|\n",
       "|1.0|5|\n",
       "|2.0|9|\n",
       "|2.5|1|\n",
       "|3.0|5|\n",
       "|3.5|1|\n",
       "|4.0|32|\n",
       "|4.5|1|\n",
       "|5.0|3|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Preliminarily accepted (median score >= 4): 36\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Rejected (median score < 3): 15\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Authors with more than one accepted submission:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Name|#|\n",
       "|--|--|\n",
       "|Paolo Melchiorre|2|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Submissions without consensus (scores range >= 3)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Total: 34\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Range|Median|Min (# reviewers)|Max (# reviewers)|# reviewers|\n",
       "|---|---|---|---|---|\n",
       "|4.0|4.0|1.0 (1)|5.0 (1)|6|\n",
       "|4.0|4.0|1.0 (1)|5.0 (1)|5|\n",
       "|4.0|3.0|1.0 (1)|5.0 (1)|6|\n",
       "|4.0|3.0|1.0 (1)|5.0 (1)|5|\n",
       "|4.0|3.0|1.0 (1)|5.0 (1)|5|\n",
       "|4.0|3.0|1.0 (1)|5.0 (1)|5|\n",
       "|3.0|4.0|2.0 (2)|5.0 (1)|5|\n",
       "|3.0|4.0|2.0 (1)|5.0 (2)|5|\n",
       "|3.0|4.0|2.0 (1)|5.0 (1)|5|\n",
       "|3.0|4.0|2.0 (1)|5.0 (1)|5|\n",
       "|3.0|4.0|1.0 (1)|4.0 (4)|5|\n",
       "|3.0|4.0|1.0 (1)|4.0 (4)|5|\n",
       "|3.0|4.0|1.0 (1)|4.0 (3)|5|\n",
       "|3.0|4.0|1.0 (1)|4.0 (3)|5|\n",
       "|3.0|4.0|1.0 (1)|4.0 (3)|5|\n",
       "|3.0|3.5|1.0 (1)|4.0 (3)|6|\n",
       "|3.0|3.0|2.0 (2)|5.0 (1)|5|\n",
       "|3.0|3.0|2.0 (2)|5.0 (1)|5|\n",
       "|3.0|3.0|2.0 (1)|5.0 (1)|5|\n",
       "|3.0|3.0|2.0 (1)|5.0 (1)|5|\n",
       "|3.0|3.0|1.0 (1)|4.0 (2)|6|\n",
       "|3.0|3.0|1.0 (1)|4.0 (2)|5|\n",
       "|3.0|3.0|1.0 (1)|4.0 (2)|5|\n",
       "|3.0|3.0|1.0 (1)|4.0 (1)|6|\n",
       "|3.0|3.0|1.0 (1)|4.0 (1)|6|\n",
       "|3.0|3.0|1.0 (1)|4.0 (1)|5|\n",
       "|3.0|3.0|1.0 (1)|4.0 (1)|5|\n",
       "|3.0|3.0|1.0 (1)|4.0 (1)|5|\n",
       "|3.0|3.0|1.0 (1)|4.0 (1)|5|\n",
       "|3.0|2.0|2.0 (3)|5.0 (1)|5|\n",
       "|3.0|2.0|1.0 (2)|4.0 (1)|6|\n",
       "|3.0|2.0|1.0 (1)|4.0 (1)|6|\n",
       "|3.0|2.0|1.0 (1)|4.0 (1)|5|\n",
       "|3.0|2.0|1.0 (1)|4.0 (1)|5|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarise_md(talks_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b05ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Workshops"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Total number of submissions: 22\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Submissions with consensus (all scores within one point)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Total: 5\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Median score|Count|\n",
       "|--|--|\n",
       "|2.0|1|\n",
       "|4.0|3|\n",
       "|5.0|1|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Preliminarily accepted (median score >= 4): 4\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Rejected (median score < 3): 1\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Authors with more than one accepted submission: None"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Submissions without consensus (scores range >= 3)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Total: 7\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|Range|Median|Min (# reviewers)|Max (# reviewers)|# reviewers|\n",
       "|---|---|---|---|---|\n",
       "|4.0|3.5|1.0 (1)|5.0 (1)|6|\n",
       "|3.0|4.0|2.0 (1)|5.0 (1)|6|\n",
       "|3.0|4.0|1.0 (1)|4.0 (3)|5|\n",
       "|3.0|3.0|1.0 (2)|4.0 (2)|5|\n",
       "|3.0|2.5|1.0 (1)|4.0 (1)|6|\n",
       "|3.0|2.0|1.0 (1)|4.0 (2)|5|\n",
       "|3.0|1.0|1.0 (3)|4.0 (1)|5|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarise_md(workshops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c00b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df(df):\n",
    "    df = df.sort_values(\"median\", ascending=False)\n",
    "    reviewer_cols = [col for col in df.columns if col.startswith(\"reviewer_\")]\n",
    "    submission_type = df.iloc[0].submission_type.lower()\n",
    "    cols = [\"title\", \"speakers\", \"category\", \"url\", \"median\", \"min\", \"max\", \"range\", \"decision_prelim\", *reviewer_cols]\n",
    "    if submission_type == \"workshop\":\n",
    "        cols.insert(2, \"duration\")\n",
    "    export_df = df[cols] \n",
    "    export_df.to_csv(f\"dce2023_{submission_type}s_reviews.csv\", index=False)\n",
    "    \n",
    "    disagreed_export_df = disagreed_df(export_df)\n",
    "    disagreed_export_df.to_csv(f\"dce2023_{submission_type}s_reviews_without_consensus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520cf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(talks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20f25669",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df(workshops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82a111f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reviews(df):\n",
    "    reviewer_cols = [col for col in df.columns if col.startswith(\"reviewer_\")]\n",
    "    plt.hist([df[col] for col in reviewer_cols], rwidth=2)\n",
    "    plt.xticks(ticks=[1, 2, 3, 4, 5])\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"# submissions\")\n",
    "    plt.legend(reviewer_cols)\n",
    "    plt.title(f\"{df.iloc[0].submission_type}s\")\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded8f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
